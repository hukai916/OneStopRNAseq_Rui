import pandas as pd
import sys, re, os, math
from snakemake.utils import min_version
from osr import gunzip, read_length
from osr import get_contrast_fnames, get_contrast_groups, get_dict_from_meta, G2B_workflow
from osr import DESeq2_input, input_rnk_fname1, input_rnk_fname2, \
Merge_TE_and_Gene_input, strand_detection_input
from osr import get_strandness, get_strandness_for_dexseq, get_strandness_for_hisat2_PE, \
get_strandness_for_hisat2_SE, get_strandness_for_stringtie

###  Usage Requirements  ###
# inputs in ./fastq/
# named as {sample}.R1.fastq.gz {sample}.R2.fastq.gz for PE reads
# {sample}.fastq.gz for SE reads

min_version("5.17.0")  # check snakemake version
configfile: "config.yaml"
container: "docker://continuumio/miniconda3:4.4.10"


def GSEA_OUTPUT(config):
    if config["GSEA_ANALYSIS"]:
        if config["START"] in ["FASTQ", "BAM", "COUNT"]:
            return(expand("gsea/{contrast}/{db}.GseaPreranked/index.html", contrast=CONTRASTS_DE, db=config["GSEA_DBS"]))
        else:
            return(expand("gsea/{contrast}/{db}.GseaPreranked/index.html", contrast=config["RNKS"], db=config["GSEA_DBS"]))
    else:
        return("Workflow_DAG.all.svg")

# umcompress files (todo: make it a rule)
if config["GENOME"].endswith(".gz"):
    gunzip(config["GENOME"])
    config["GENOME"] = re.sub(".gz$", "", config["GENOME"])

if config["GTF"].endswith(".gz"):
    gunzip(config["GTF"])
    config["GTF"] = re.sub(".gz$", "", config["GTF"])

if config["VCF"].endswith(".gz"):
    gunzip(config["VCF"])
    config["VCF"] = re.sub(".gz$", "", config["VCF"])


# params todo: check yaml format
SAMPLES=config['SAMPLES']
PAIR_END=config['PAIR_END']
GENOME=config["GENOME"]
INDEX=config["INDEX"]
GTF=config["GTF"]
ANNO_TAB=config["ANNO_TAB"]
STRAND=config["STRAND"]
MODE=config["MODE"]
MAX_FDR=config["MAX_FDR"]
MIN_LFC=config["MIN_LFC"]
MIN_GENE_COUNT= config["MIN_GENE_COUNT"] if 'MIN_GENE_COUNT' in config else 100  # DEXSeq
CONTRAST_AS = config['CONTRAST_AS']
META = config['META']


# for DESeq2
if config['DESEQ2_ANALYSIS'] and config['START'] in ["FASTQ", "BAM", "COUNT"]:
    CONTRASTS_DE = get_contrast_fnames(config['CONTRAST_DE'])
    CONTRASTS_DE = [x.replace("-", '.') for x in CONTRASTS_DE]
else:
    CONTRASTS_DE = ["placeholder"]

# for DEXSeq
if config['DEXSEQ_ANALYSIS'] and config["START"] in ["FASTQ", "BAM"]:
    CONTRASTS_AS = get_contrast_fnames(config['CONTRAST_AS'])
else:
    CONTRASTS_AS = ["placeholder"]
CONTRASTS_AS = [l.replace('.','_') for l in CONTRASTS_DE]


# for rMATS
if config['RMATS_ANALYSIS'] and config['START'] in ["FASTQ", "BAM"]:
    G = get_contrast_groups(CONTRAST_AS)
    #print("\nGRPUPS:\n", G[0], "\nVS\n", G[1])

    g2s = get_dict_from_meta(config['META'])
    #print("\nGroup To Sample Mapping:\n", g2s)

    B1S, B1 = G2B_workflow(G[0], g2s)
    B2S, B2 = G2B_workflow(G[1], g2s)

    # print("\nrMATS -b1:\n", B1)
    # print("B1S:", B1S)
    # print("rMATS -b2:\n", B2)
    # print("B2S:", B2S)

    # Get contrast for rMATS
    df2 = pd.read_excel(CONTRAST_AS, engine='openpyxl')
    ASCN = df2.shape[1] # alternative splicing contrast count
    #print("ASCN", ASCN)
else:
    B1 = "placeholder.bam"
    B1S = ["placeholder"]
    B2 = B1
    B2S = B1S
    ASCN = 0


def target_input(config, ASCN):
    '''
    - define target results to generate
    - check config logic error (in-compatibility)
    '''
    l = ["Workflow_DAG.all.svg"]

    if config['START'] == 'FASTQ':
        l.append('fastqc/multiqc_report.html')

    if config['START'] in ['FASTQ', 'BAM']:
        l.append('bam_qc/QoRTs_MultiPlot/plot-basic.pdf')
        l.extend([
            'bam_qc/STAR_Align_summary_multiqc_report.html',
            'bam_qc/samtools_stats_multiqc_report.html'])
        l.extend(['bigWig/{}.{}.cpm.bw'.format(sample, config['MODE']) \
                for sample in config['SAMPLES']])

    if config['DESEQ2_ANALYSIS']:
        if config['START'] not in ['FASTQ', 'BAM', 'COUNT']:
            raise Exception ('config Error: START and DESEQ2_ANALYSIS incompatible')
        l.append("DESeq2/DESeq2.html")

    if config['GSEA_ANALYSIS']:
        l.extend(GSEA_OUTPUT(config))

    if config['RMATS_ANALYSIS']:
        if config['START'] not in ['FASTQ', 'BAM']:
            raise Exception ('config Error: START and RMATS_ANALYSIS incompatible')
        l.extend(['rMATS.{ascn}/output/Results_JunctionCountsBased/RI.MATS.JC.txt'.\
            format(ascn=ascn) for ascn in range(1, 1+ASCN)])

    if config['DEXSEQ_ANALYSIS']:
        if config['START'] not in ['FASTQ', 'BAM']:
            raise Exception ('config Error: START and DEXSEQ_ANALYSIS incompatible')
        l.append(["DEXSeq/contrast{ascn}/contrast{ascn}.RData".\
            format(ascn=ascn) for ascn in range(1,ASCN+1)])

    if config["ASSEMBLY_ANALYSIS"]:
        if config['START'] not in ['FASTQ']:
            raise Exception ('config Error: START and ASSEMBLY_ANALYSIS incompatible')
        l.append('stringtie/stringtie.merged.gtf')

    # todo: fix GATK env
    if config['ASE_ANALYSIS']:
        if config['START'] not in ['FASTQ', 'BAM']:
            raise Exception ('config Error: START and ASE_ANALYSIS incompatible')
        l.extend(["GATK_ASEReadCounter/{sample}.table".\
            format(sample=sample) for sample in SAMPLES])

    return (l)

rule targets:
    input:
        target_input(config, ASCN)
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000   


rule FastQC:
    input:
        ["fastq/{sample}.R1.fastq.gz", "fastq/{sample}.R2.fastq.gz"] if PAIR_END else \
        "fastq/{sample}.fastq.gz" 
    output:
        ["fastqc/details/{sample}.R1_fastqc.html", "fastqc/details/{sample}.R2_fastqc.html"] if PAIR_END else \
        "fastqc/details/{sample}_fastqc.html",
        ["fastqc/details/{sample}.R1_fastqc.zip", "fastqc/details/{sample}.R2_fastqc.zip"] if PAIR_END else \
        "fastqc/details/{sample}_fastqc.zip"
    conda:
        "envs/fastqc.yaml"
    priority:
        10
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000   
    threads:
        1
    log:
        "log/fastqc/{sample}.log"
    benchmark:
        "log/fastqc/{sample}.benchmark"
    shell:
        "which fastqc &> {log};"
        "fastqc -t {threads} {input} -o fastqc/details &>> {log};"


rule FastQC_MultiQC:
    input:
        [expand("fastqc/details/{sample}.R1_fastqc.zip", sample=SAMPLES), 
        expand("fastqc/details/{sample}.R2_fastqc.zip", sample=SAMPLES)] \
        if PAIR_END else \
        expand("fastqc/details/{sample}_fastqc.zip", sample=SAMPLES)
    output:
        "fastqc/multiqc_report.html"
    conda:
        "envs/fastqc.yaml"
    priority:
        10
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000   
    threads:
        1
    log:
        "log/multiqc/multiqc.log"
    benchmark:
        "log/multiqc/multiqc.benchmark"
    shell:
        """
        which multiqc &> {log};
        multiqc {input} -f -o fastqc &>> {log};
        D=fastqc; rm  -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ &>> {log};
        """


rule STAR_Index:
    input:
        fa=GENOME,
        gtf=GTF,
    output:
        INDEX+"/SAindex"
    conda:
        "envs/star.yaml"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 6400   
    threads:
        6
    log:
        "log/star_idx/star_idx.log"
    benchmark:
        "log/star_idx/star_idx.benchmark"
    # envmodules:
    #     "star/2.7.5a"
    shell:
        """
        whoami > {log};
        which STAR >> {log};

        STAR --runThreadN {threads} \
        --runMode genomeGenerate \
        --genomeDir {INDEX} \
        --genomeFastaFiles {input.fa} \
        --sjdbGTFfile {input.gtf} &>> {log}
        """

rule STAR:
    input:
        index=INDEX+"/SAindex",
        gtf=GTF,
        reads=  ["fastq/{sample}.R1.fastq.gz","fastq/{sample}.R2.fastq.gz"] \
                if PAIR_END else \
                "fastq/{sample}.fastq.gz"
    output:
        log="mapped_reads/{sample}.Log.final.out",
        bam=temp("mapped_reads/{sample}.bam"),
    conda:
        "envs/star.yaml"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 3000 
    threads:
        12
    log:
        "log/mapped_reads/{sample}.star.log"
    benchmark:
        "log/mapped_reads/{sample}.star.benchmark"
    # envmodules:
    #     "star/2.7.5a"
    shell:
        """
        which STAR > {log};

        STAR --runThreadN {threads} --genomeDir {INDEX} --sjdbGTFfile {input.gtf} \
        --readFilesCommand zcat --readFilesIn {input.reads} \
        --outFileNamePrefix mapped_reads/{wildcards.sample}. \
        --outFilterType BySJout \
        --outMultimapperOrder Random \
        --outFilterMultimapNmax 200 \
        --alignSJoverhangMin 8 \
        --alignSJDBoverhangMin 3 \
        --outFilterMismatchNmax 999 \
        --outFilterMismatchNoverReadLmax 0.05 \
        --alignIntronMin 20 \
        --alignIntronMax 1000000 \
        --alignMatesGapMax 1000000 \
        --outFilterIntronMotifs RemoveNoncanonicalUnannotated \
        --outSAMstrandField None \
        --outSAMtype BAM Unsorted \
        --quantMode GeneCounts \
        --outReadsUnmapped Fastx \
        >> {log} 2>&1

        mv mapped_reads/{wildcards.sample}.Aligned.out.bam mapped_reads/{wildcards.sample}.bam &>> {log}
        pigz -p {threads} -f mapped_reads/{wildcards.sample}.Unmapped.out.mate*  &>> {log}
        """

       
rule STAR_MultiQC:
    input:
        expand("mapped_reads/{sample}.Log.final.out", sample=SAMPLES)
    output:
        expand("bam_qc/STAR_Align_summary/{sample}.Log.final.out", sample=SAMPLES),
        "bam_qc/STAR_Align_summary_multiqc_report.html",    
    conda:
        "envs/fastqc.yaml"
    resources:
        mem_mb="2000"
    threads:
        1
    log:
        'log/bam_qc/STAR_Align_summary_multiqc_report.log'
    benchmark:
        'log/bam_qc/STAR_Align_summary_multiqc_report.benchmark'
    shell:
        """
        mkdir -p bam_qc/STAR_Align_summary/
        cp {input} bam_qc/STAR_Align_summary/

        multiqc {input} -f --outdir bam_qc --title STAR_Align_summary &> {log}
        """


rule Samtools_Sort:
    input:
        "mapped_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam"
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000 
    threads:
        4
    log:
        "log/samtools_sort/{sample}.sort.log"
    benchmark:
        "log/samtools_sort/{sample}.sort.benchmark"
    shell:
        "which samtools &> {log};"
        "samtools sort -@ {threads} -m 3G {input} -o {output} &>> {log}"


rule Samtools_Index:
    input:
        "sorted_reads/{sample}.bam"
    output:
        "sorted_reads/{sample}.bam.bai"
    conda:
        "envs/samtools.yaml"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2000
    threads:
        1
    log:
        "log/samtools_index/{sample}.index.log"
    benchmark:
        "log/samtools_index/{sample}.index.benchmark"
    shell:
        "which samtools &> {log};"
        "samtools index -@ {threads} {input} &>> {log}"


rule Samtools_QC:
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        stats="bam_qc/stats/{sample}.stats.txt",
        idxstats="bam_qc/idxstats/{sample}.idxstats.txt",
    conda:
        "envs/samtools.yaml"
    priority:
        0
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2000
    threads:
        2
    log:
        idxstats="log/bam_qc/idxstats/{sample}.idxstats.log",
        stats="log/bam_qc/stats/{sample}.stats.log",
    benchmark:
        "log/bam_qc/benchmark/{sample}.bam_qc.benchmark",
    shell:
        """
        samtools idxstats {input.bam} > {output.idxstats} 2> {log.idxstats} &
        samtools stats {input.bam} > {output.stats} 2> {log.stats} &

        wait
        """


rule Samtools_QC_MultiQc:
    input:
        stats=expand("bam_qc/stats/{sample}.stats.txt", sample=SAMPLES),
        idxstats=expand("bam_qc/idxstats/{sample}.idxstats.txt", sample=SAMPLES)
    output:
        "bam_qc/samtools_stats_multiqc_report.html",    
        "bam_qc/samtools_idxstats_multiqc_report.html",   
    conda:
        "envs/fastqc.yaml"
    resources:
        mem_mb="2000"
    threads:
        1
    log:
        'log/bam_qc/samtools_multiqc_report.log'
    benchmark:
        'log/bam_qc/samtools_multiqc_report.benchmark'
    shell:
        """
        multiqc {input.stats} -f --outdir bam_qc --title samtools_stats &> {log}
        multiqc {input.idxstats} -f --outdir bam_qc --title samtools_idxstats &>> {log}
        """


rule QoRTs:
    input:
        bam="sorted_reads/{sample}.bam",
        gtf=config['GTF']
    output:
        "bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK"
    params:
        pe = ' ' if PAIR_END else '--singleEnded'
    conda:
        "envs/java11.yaml"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16500
    threads:
        1
    log:
        "log/QoRTs/QoRTs.{sample}.log"
    benchmark:
        "log/QoRTs/QoRTs.{sample}.benchmark"
    shell:
        """
        which java &> {log}

        java -Xmx16G -jar workflow/envs/hartleys-QoRTs-099881f/QoRTs.jar QC {params.pe} \
        {input.bam} {input.gtf} bam_qc/QoRTs/{wildcards.sample}/ &>> {log}
        """


rule QoRTs_MultiPlot:
    input:
        expand("bam_qc/QoRTs/{sample}/QC.QORTS_COMPLETED_OK", sample=SAMPLES)
    output:
        "bam_qc/QoRTs_MultiPlot/plot-basic.pdf"
    conda:
        "envs/qorts.yaml"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 8000   ,
    threads:
        2
    log:
        "log/QoRTs/multiplot.log"
    benchmark:
        "log/QoRTs/multiplot.benchmark"
    shell:
        """
        which python && which Rscript &> {log};
        python workflow/script/meta_to_decoder.py &>> {log};
        Rscript workflow/script/QoRT.R &>> {log}; # needs QoRT package in R
        mkdir -p bam_qc/QoRTs_MultiPlot/details/;
        mv bam_qc/QoRTs_MultiPlot/plot-sample* bam_qc/QoRTs_MultiPlot/details/;

        D=bam_qc  ; # todo: more robust rule in case other bam_qc finishes faster (highly unlikely)
        rm -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ 
        """


rule bamCoverage:
    input:
        bam="sorted_reads/{sample}.bam",
        bai="sorted_reads/{sample}.bam.bai"
    output:
        "bigWig/{sample}.strict.cpm.bw" if MODE == 'strict' else \
        "bigWig/{sample}.liberal.cpm.bw"
    params:
        "--minMappingQuality 20" if MODE == "strict" else " "
    conda:
        "envs/deeptools.yaml" 
    threads:
        4
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,
    priority:
        0
    log:
        "log/bamCoverage/{sample}.bamCoverage.log"
    benchmark:
        "log/bamCoverage/{sample}.bamCoverage.benchmark"
    shell:
        """
        which bamCoverage &> {log}

        bamCoverage --bam {input.bam} -o  {output} --numberOfProcessors {threads} \
        --outFileFormat bigwig --normalizeUsing CPM --binSize 50 \
        {params} &>> {log}
        """


rule featureCounts:
    '''
    exon read count
    recognizes config.yaml: 
        - MODE: strict, liberal
        - PAIR_END: True, False
    '''
    input:
        bams=expand("mapped_reads/{sample}.bam", sample=SAMPLES), # sort by name -> faster
        gtf=GTF
    output:
        COUNT="feature_count/counts.s{strand}.strict.txt" \
                if MODE == "strict" else \
                "feature_count/counts.s{strand}.liberal.txt",
        summary="feature_count/counts.s{strand}.strict.txt.summary" \
                if MODE == "strict" else \
                "feature_count/counts.s{strand}.liberal.txt.summary",
    params:
        pe = '-p' if PAIR_END else ' ', 
        mode = '-Q 20 --minOverlap 1 --fracOverlap 0 -B -C' \
                if MODE == 'strict' else \
                '-M -Q 0 --primary --minOverlap 1 --fracOverlap 0'
    conda:
        "envs/subread.yaml" 
    priority:
        100
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,
    threads:
        4
    log:
        "log/feature_count/counts.s{strand}.log"
    benchmark:
        "log/feature_count/benchmark.s{strand}.benchmark"
    shell:
        """
        which featureCounts &> {log}
        featureCounts -a {input.gtf} -o {output.COUNT} \
        -T {threads} -g gene_id -s {wildcards.strand} \
        {params.pe}  {params.mode} \
        {input.bams} &>> {log}
        """


rule featureCounts_GeneLevel:
    '''
    gene read count, including exon and intron reads
    the larger one of gene-count/exon-count for each gene, is used as gene-count
    recognizes config.yaml: 
        - MODE: strict, liberal
        - PAIR_END: True, False
    '''
    input:
        bams=expand("mapped_reads/{sample}.bam", sample=SAMPLES), # sort by name -> faster
        exon_counts="feature_count/counts.s{strand}.strict.txt" \
                        if config["MODE"] == "strict" else \
                    "feature_count/counts.s{strand}.liberal.txt",
        gtf=GTF
    output:
        ct = 'feature_count_gene_level/counts.s{strand}.strict.txt' \
                    if config["MODE"] == "strict" else \
                'feature_count_gene_level/counts.s{strand}.liberal.txt',
        summary = 'feature_count_gene_level/counts.s{strand}.strict.txt.summary' \
                    if config["MODE"] == "strict" else \
                 'feature_count_gene_level/counts.s{strand}.liberal.txt.summary',
    params:
        pe = '-p -B -C ' if PAIR_END else ' ',
        mode = '-Q 20 ' if MODE == 'strict' else '-M --primary -Q 0'
    conda:
        "envs/subread.yaml" 
    priority:
        0
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,
    threads:
        4
    log:
        "log/feature_count_gene_level/counts.s{strand}.log"
    benchmark:
        "log/feature_count_gene_level/benchmark.s{strand}.benchmark"
    shell:
        """
        # gene level count
        featureCounts -a {input.gtf} -o {output.ct} -T {threads} \
        -t gene -g gene_id -s {wildcards.strand} \
        --minOverlap 1 --fracOverlap 0 \
        {params.pe} {params.mode} {input.bams} \
        > {log} 2>&1

        # get larger count (exon vs gene) for each gene
        python workflow/script/get_max_of_featureCountsTable.py \
        {input.exon_counts} {output.ct}  \
        >> {log} 2>&1
        """





rule Strand_Detection:
    input:
        strand_detection_input(config)
    output:
        "feature_count/counts.strict.txt" \
        if config["MODE"] == "strict" \
        else  "feature_count/counts.liberal.txt",
        "meta/strandness.detected.txt"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    log:
        "log/strand_detection.log"
    benchmark:
        "log/strand_detection.benchmark"
    script:
        "script/strandness_detection.py"



rule Strand_Detection_GeneLevel:
    input:
        expand("feature_count_gene_level/counts.s{strand}.strict.txt.summary", strand=STRAND) 
        if config["MODE"] == "strict" 
        else expand("feature_count/counts.s{strand}.liberal.txt.summary", strand=STRAND)
    output:
        "feature_count_gene_level/counts.strict.txt" \
        if config["MODE"] == "strict" \
        else  "feature_count_gene_level/counts.liberal.txt",
        "feature_count_gene_level/strandness.detected.txt"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000
    log:
        "log/feature_count_gene_level/strand_detection.log"
    benchmark:
        "log/feature_count_gene_level/strand_detection.benchmark"
    script:
        "script/strandness_detection.ge.py"


# rule SalmonTE_prep
if config['PAIR_END']:
    rule SalmonTE_Prep:
        input:
            r1=expand("fastq/{sample}.R1.fastq.gz", sample=SAMPLES),
            r2=expand("fastq/{sample}.R2.fastq.gz", sample=SAMPLES)    
        output:
            r1=expand("fastq_salmon/{sample}_1.fastq.gz", sample=SAMPLES),
            r2=expand("fastq_salmon/{sample}_2.fastq.gz", sample=SAMPLES)
        log:
            "fastq_salmon/SalmonTE_prep.log"
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1000
        threads:
            1
        run:
            for i,f in enumerate(input):
                os.system("ln -s "+"../"+input[i] + " " + output[i])


rule SalmonTE:
    # only start from Fastq
    input:
        reads=  [expand("fastq_salmon/{sample}_1.fastq.gz", sample=SAMPLES),
                expand("fastq_salmon/{sample}_2.fastq.gz", sample=SAMPLES)] \
                if PAIR_END else \
                expand("fastq/{sample}.fastq.gz", sample=SAMPLES),
    output:
        "SalmonTE_output/EXPR.csv"
    conda:
        "envs/salmonte.yaml"  # test
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000,
    params:
        ref=config['TE_REFERENCE']
    threads:
        12
    log:
        'log/SalmonTE/salmonte_count.log'
    benchmark:
        'log/SalmonTE/salmonte_count.benchmark'
    shell:
        """
        which python &> {log}
        python workflow/envs/SalmonTE/SalmonTE.py --version >> {log}
        python workflow/envs/SalmonTE/SalmonTE.py quant \
        --reference={params.ref} --exprtype=count \
        --num_threads={threads} \
        {input.reads} >> {log} 2>&1
        """



rule Merge_TE_and_GE:
    input:
        gene=Merge_TE_and_Gene_input(config),
        te="SalmonTE_output/EXPR.csv"
    output:
        "feature_count_gene_level/TE_included.txt" \
        if config['INTRON'] \
        else "feature_count/TE_included.txt"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000,
    threads:
        1
    log:
        "log/merge_te_and_gene/merge_te_and_gene.log"
    benchmark:
        "log/merge_te_and_gene/merge_te_and_gene.benchmark"
    shell:
        """
        python workflow/script/merge_featureCount_and_SalmonTE.py \
        {input.gene} {input.te} {output} > {log} 2>&1
        """

rule DESeq2:
# todo: use softlinked workflow/script/DESeq2.rmd, use softlinked example_data/ at COUNT-START
    input:
        cnt=DESeq2_input(config),
        meta=META,
        contrast=config["CONTRAST_DE"],
    output:
        "DESeq2/DESeq2.html",
        expand("DESeq2/rnk/{contrast}.rnk", contrast=CONTRASTS_DE)
    conda:
        "envs/deseq2.yaml"  # test
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,
    params:
        rmd="'./DESeq2/DESeq2.Rmd'",
        fdr=MAX_FDR,
        lfc=MIN_LFC,
        independentFilter=config["independentFilter"],
        cooksCutoff=config["cooksCutoff"],
        blackSamples=config['blackSamples'] if 'blackSamples' in config else "", 
        anno_tab=ANNO_TAB,
        o="'DESeq2.html'"
    priority: 
        100
    threads:
        1
    log:
        "DESeq2/DESeq2.log"
    benchmark:
        "DESeq2/DESeq2.benchmark"
    shell:
        #Rscript -e rmarkdown::render({params.rmd})
        'cp workflow/script/DESeq2.Rmd DESeq2; '
        'Rscript -e "rmarkdown::render( \
        {params.rmd}, \
        params=list( \
            max_fdr={params.fdr}, \
            min_lfc={params.lfc}, \
            cookscutoff={params.cooksCutoff}, \
            indfilter={params.independentFilter}, \
            countFile=\'../{input.cnt}\', \
            annoFile=\'{params.anno_tab}\', \
            metaFile=\'../{input.meta}\', \
            contrastFile=\'../{input.contrast}\', \
            blackSamples=\'{params.blackSamples}\' \
            ), \
        output_file={params.o})" > {log} 2>&1 ;'
        'D=DESeq2; rm -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ &>> {log};'




rule GSEA:
    """
    config[START] == RNK, then find config[RNKS], else, find {contrast} 
    config[RNKS] contains the full name of the rank file (xlsx or txt), must be put in ./meta/ folder
    rnk file must be named xxx.rnk.txt when feeded to GSEA

    # todo: 
    [x] no svg if nplots > 200 (yes)
    [] 
    """
    input: 
        deseq2= "Workflow_DAG.all.svg" # dummy assume existing
            if config["START"] == "RNK"
            else "DESeq2/DESeq2.html" , # DESEq2, then GSEA
        rnk=lambda wildcards: input_rnk_fname1(wildcards, config),
        db="meta/gsea_db/{db}",
    output:
        "gsea/{fname}/{db}.GseaPreranked/index.html"
    conda:
        "envs/java11.yaml"  # test
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 24000,
    params:
        #svg=config["GSEA_PLOT_SVG"] if config["GSEA_NPLOTS"] <= 200 else False,  # no svg if nplots > 200 (to save time and space) # todo: move this to front end, when making workflow portable
        svg=config["GSEA_PLOT_SVG"],
        nplot=config["GSEA_NPLOTS"],
        label_db=lambda wildcards: wildcards["db"][:-12],
        rnk_flat_file= lambda wildcards: input_rnk_fname2(wildcards, config),
    threads:
        1
    log:
        "log/gsea/{fname}.{db}.log"
    benchmark:
        "log/gsea/{fname}.{db}.benchmark"
    priority:
        100
    shell:
        """
        echo fname1: {input.rnk} > {log}
        echo fname2: {params.rnk_flat_file} >> {log}
        echo label_db: {params.label_db} >> {log}
        echo wildcards.fname: {wildcards.fname} >> {log}
        echo wildcards.db: {wildcards.db} >> {log}

        mkdir -p gsea && mkdir -p gsea/{wildcards.fname} &>> {log}
        rm -rf gsea/{wildcards.fname}/{wildcards.db}.GseaPreranked.*/  # avoid confusion
        rm -rf gsea/{wildcards.fname}/{wildcards.db}.GseaPreranked/  # avoid dir structure mistake
        rm -rf gsea/{wildcards.fname}/error_{wildcards.db}*GseaPreranked*/  # avoid confusion of temp files in multiple run attempts

        python workflow/script/fix_gsea_input.py {input.rnk} &>> {log}  # fix gene symbol error; change test.xlsx to test.rnk.txt
        
        which java &>> {log}

        if workflow/envs/GSEA_4.0.3/gsea-cli.sh GSEAPreranked \
        -gmx {input.db} -rnk {params.rnk_flat_file} -rpt_label {wildcards.db} \
        -norm meandiv -nperm 1000  -scoring_scheme classic \
        -create_svgs {params.svg} -make_sets true  -rnd_seed timestamp -zip_report false \
        -set_max 15000 -set_min 15 \
        -plot_top_x {params.nplot} -out ./gsea/{wildcards.fname} &>> {log}
        then
            mv gsea/{wildcards.fname}/{wildcards.db}.GseaPreranked.*/ \
            gsea/{wildcards.fname}/{wildcards.db}.GseaPreranked/  &>> {log} 
        else
            mv gsea/{wildcards.fname}/*{wildcards.db}.GseaPreranked*/ \
            gsea/{wildcards.fname}/{wildcards.db}.GseaPreranked/  &>> {log}
        fi
    
        cp workflow/envs/GSEA_MSigDB_ReadMe.html gsea/ &>> {log}

        # compress
        echo compress
        D=gsea/{wildcards.fname}/{wildcards.db}.GseaPreranked
        rm -f $D.zip && [ -d $D ] && zip -rq $D.zip $D/ &>> {log}
        """

# todo: if gsea/contrast/err_{db}, touch gsea/contrast/db.finished

rule Read_Length_Detection:
    input:
        expand("bam_qc/stats/{sample}.stats.txt", sample=SAMPLES)
            if config['START'] in ["FASTQ", "BAM"]
            else "BAM file not provided. AND can't be generated from FASTQ, because FASTQ not provided"
    output:
        "meta/read_length.txt"
    resources:
        mem_mb="1000",
    threads:
        1
    log:
        "log/read_length_detection_bam/read_length_detection_bam.log"
    benchmark:
        "log/read_length_detection_bam/read_length_detection_bam.benchmark"
    run:
        import re
        import statistics
        lengths = []
        for f in input:
            p = re.compile("average length:\s*(\d*)")
            for i, line in enumerate(open(f, "r")):
                for match in re.finditer(p, line):
                    lengths.append(match.group(1))
        lengths = list(map(int, lengths))
        print("read lengths detected from BAM:", lengths,"\n")
        if(len(set(lengths)) < 1):
            sys.exit("read lengths detection from BAM failed")
        if(len(set(lengths)) > 1):
            print("Not all fastq files have the same length, will give rMATS the median length")
        
        median_length = int(statistics.median(lengths))
        print(output[0])
        with open(output[0], "w") as out:
            out.write(str(median_length))


rule rMATS:
    #todo: update to docker: simple to install, faster, recommended on website
    input:
        b1=lambda wildcards: B1S[int(wildcards['ascn'])-1],
        b2=lambda wildcards: B2S[int(wildcards['ascn'])-1],
        gtf=GTF, 
        length_file="meta/read_length.txt" , 
        strand_file="meta/strandness.detected.txt",
    output:
        "rMATS.{ascn}/output/Results_JunctionCountsBased/RI.MATS.JC.txt"
    envmodules:
        "rMATS/4.1.0 "
        "gcc/8.1.0 " # must have space
    container:
        'envs/rmats.sif'
    conda:
       "envs/rmats.yaml"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000,  # todo reduce
    threads:
        4
    params:
        b1=lambda wildcards: B1[int(wildcards['ascn'])-1], 
        b2=lambda wildcards: B2[int(wildcards['ascn'])-1], 
        type="paired" if config['PAIR_END'] else "single", 
        analysis="P" if config['PAIR_END'] else "U", 
        length=read_length("meta/read_length.txt"), 
        strandness=get_strandness("meta/strandness.detected.txt", config),
        MAX_FDR=MAX_FDR
    log:
        "log/rMATS/rMATS.{ascn}.log"
    benchmark:
        "log/rMATS/rMATS.{ascn}.benchmark"
    shell:
        """
        rm -rf rMATS.{wildcards.ascn}
        mkdir -p rMATS.{wildcards.ascn}/
        mkdir -p rMATS.{wildcards.ascn}/params/
        echo {params.b1} > rMATS.{wildcards.ascn}/params/b1.txt
        echo {params.b2} > rMATS.{wildcards.ascn}/params/b2.txt
        which python &> {log}
        rmats.py  \
        --b1 rMATS.{wildcards.ascn}/params/b1.txt \
        --b2  rMATS.{wildcards.ascn}/params/b2.txt \
        --gtf {input.gtf} \
        -t {params.type} \
        --readLength {params.length} \
        --variable-read-length \
        --libType {params.strandness} \
        --nthread {threads} \
        --tstat {threads} \
        --cstat 0.2 \
        --od rMATS.{wildcards.ascn}/output/ \
        --tmp rMATS.{wildcards.ascn}/tmp/  &>> {log}
        cp workflow/envs/rMATS_ReadMe.html rMATS.{wildcards.ascn}/output/  &>>{log}
        rm -rf rMATS.{wildcards.ascn}/tmp/   &>> {log}
        mkdir -p rMATS.{wildcards.ascn}/output/Results_JunctionCountsBased/ &>> {log}
        mkdir -p rMATS.{wildcards.ascn}/output/Results_JunctionCountsAndExonCountsBased/  &>> {log}
        mkdir -p rMATS.{wildcards.ascn}/output/fromGTF/ &>> {log}
        mv rMATS.{wildcards.ascn}/output/*JCEC.txt rMATS.{wildcards.ascn}/output/Results_JunctionCountsAndExonCountsBased/ &>> {log}
        mv rMATS.{wildcards.ascn}/output/*JC.txt rMATS.{wildcards.ascn}/output/Results_JunctionCountsBased/ &>> {log}
        mv rMATS.{wildcards.ascn}/output/fromGTF*txt  rMATS.{wildcards.ascn}/output/fromGTF/ &>> {log}
        rm -f rMATS.{wildcards.ascn}/output/*raw.input* &>> {log}
        D=rMATS.{wildcards.ascn};
        rm -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ &>> {log};
        """


DEXSeq_GFF = config['GTF']+".dexseq.gff"

rule DEXSeq_GFF_Prep:
    input:
        config['GTF']
    output:
        DEXSeq_GFF
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000
    threads:
        1
    log:
        "log/DEXSeq/prep_gff.log"
    benchmark:
        "log/DEXSeq/prep_gff.benchmark"
    conda:
        "envs/bioconda_misc.yaml"
    shell:
        "python workflow/script/dexseq_prepare_annotation.py -r no {input} {output} &> {log}"




rule DEXSeq_Count:
    input:
        bam="sorted_reads/{sample}.bam",
        strandFile="meta/strandness.detected.txt", 
        gff=DEXSeq_GFF
    output:
        "DEXSeq_count/{sample}_count.txt"
    conda:
        "envs/bioconda_misc.yaml"
    params:
        strand=get_strandness_for_dexseq('meta/strandness.detected.txt'),
        readType=('-p yes -r pos'
            if config['PAIR_END']
            else ' '), 
        MAPQ=10
    resources:
         mem_mb=lambda wildcards, attempt: attempt * 16000
    log:
        "log/DEXSeq/prep_count.{sample}.log"
    benchmark:
        "log/DEXSeq/prep_count.{sample}.benchmark"
    shell:
        "python workflow/script/dexseq_count.py -f bam -a {params.MAPQ} {params.readType} {params.strand} {DEXSeq_GFF} {input.bam} {output}"  
        # todo: maybe -r name (sort by name) is faster


rule DEXSeq:
    input:
        count_files = expand("DEXSeq_count/{sample}_count.txt", sample = SAMPLES),
        meta=META,
        contrast=CONTRAST_AS,
        gffFile=config['GTF']+".dexseq.gff",
        strandness="meta/strandness.detected.txt"
    output:
        "DEXSeq/contrast{ascn}/contrast{ascn}.RData"
    conda:
        "envs/dexseq.yaml"  # test
    resources:
         mem_mb=lambda wildcards, attempt: attempt * 3000
    threads:
        12
    params:
        rmd="'workflow/script/dexseq.r'",
        annoFile=config["ANNO_TAB"],
    log:
        "log/DEXSeq/DEXSeq.{ascn}.log"
    benchmark:
        "log/DEXSeq/DEXSeq.{ascn}.benchmark"
    shell:
        """
        mkdir -p DEXSeq 
        which Rscript &> {log}
        cp workflow/script/dexseq.r DEXSeq/ &>> {log}
        Rscript workflow/script/dexseq.r {input.meta} {input.contrast} {input.gffFile} {params.annoFile} {MAX_FDR} {MIN_LFC} {threads} {MIN_GENE_COUNT} {wildcards.ascn} &>> {log}
        D=DEXSeq/contrast{wildcards.ascn}
        rm -f $D.zip && [ -d $D ] && zip -rq $D.zip $D/ &>> {log}
        """

rule Genome_Faidx:
    input:
        config['GENOME']
    output:
        config['GENOME']+'.fai'
    conda:
        "envs/samtools.yaml"  # test
    resources:
        mem_mb=lambda wildcards, attempt: attempt *4000
    threads:
        1
    log:
        "log/genome_faidx/faidx.log"
    shell:
        "samtools faidx {input}  &> {log}"


rule GATK_CreateSequenceDictionary:
    input:
        config['GENOME'],
    output:
        re.sub( "fa$", "dict", config['GENOME'])
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000
    conda:
        "envs/gatk.yaml"  # test
    threads:
        1
    log:
        "log/GATK_CreateSequenceDictionary/GATK_CreateSequenceDictionary.log"
    benchmark:
        "log/GATK_CreateSequenceDictionary/GATK_CreateSequenceDictionary.benchmark"
    shell:
        "gatk CreateSequenceDictionary -R {input} &> {log}"


rule GATK_ASEReadCounter:
    input:
        genome=config['GENOME'],
        genome_idx=config['GENOME']+'.fai',
        genome_ref=re.sub( "fa$", "dict", config['GENOME']),
        bam="sorted_reads/{sample}.bam",
        vcf=config['VCF']
    output:
        table="GATK_ASEReadCounter/{sample}.table"
    conda:
        "envs/gatk.yaml"  # test
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 16000
    threads:
        1
    log:
        "log/GATK_ASEReadCounter/{sample}.log"
    benchmark:
        "log/GATK_ASEReadCounter/{sample}.benchmark"
    shell:
        "gatk ASEReadCounter -R {input.genome} -I {input.bam} -V {input.vcf} -O {output} \
        --min-depth-of-non-filtered-base 10 --min-mapping-quality 15 --min-base-quality 20 \
        &> {log};"  # todo: more thoughts on detailed parameters, e.g.    -drf DuplicateRead -U ALLOW_N_CIGAR_READS
        "D=GATK_ASEReadCounter; "
        "rm -f $D/$D.zip && [ -d $D ] && zip -rq  $D/$D.zip $D/ &>> {log};"


rule Hisat2_Build:
    """
    guided by GTF
    large RAM needed? More threads more RAM?
    with only genome.fa, only takes ~6GB RAM, 20min
    $ hisat2-build -p 16 genome.fa genome

    with transcripts, takes 160GB RAM, 1h
    $ hisat2-build -p 16 --exon genome.exon --ss genome.ss genome.fa genome_tran
    http://daehwankimlab.github.io/hisat2/howto/
    """
    input:
        genome=config['GENOME'],
        gtf=config['GTF']
    output:
        ss=config['GTF']+"ss",
        exon=config['GTF']+"exon",
        index=config['GENOME'] + ".1.ht2",
        flag=touch(config['GENOME'] + '.hisat2_build.finished')
    conda:
        "envs/hisat2.yaml" 
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 24000
    threads:
        8
    log:
        "log/hisat2_build/hisat2_build.log"
    benchmark:
        "log/hisat2_build/hisat2_build.benchmark"
    shell:
        """
        hisat2_extract_splice_sites.py -v {input.gtf} > {output.ss} 2> {log}
        hisat2_extract_exons.py -v {input.gtf} > {output.exon} 2>> {log}
        hisat2-build -p {threads} \
        {input.genome} {input.genome} --ss {output.ss} --exon {output.exon} 2>> {log}
        """

rule Hisat2:
    input:
        genome=config['GENOME'],
        index =config['GENOME'] + '.hisat2_build.finished',
        reads=  ["fastq/{sample}.R1.fastq.gz","fastq/{sample}.R2.fastq.gz"] \
                    if PAIR_END else \
                "fastq/{sample}.fastq.gz",
        strandness="meta/strandness.detected.txt"
    output:
        bam="hisat2/{sample}.bam",
        bai="hisat2/{sample}.bam.bai"
    conda:
        "envs/hisat2.yaml" 
    params:
        strand=get_strandness_for_hisat2_PE("meta/strandness.detected.txt"), 
        pe= "-1 fastq/{sample}.R1.fastq.gz -2 fastq/{sample}.R2.fastq.gz" \
                if PAIR_END else \
            "-U fastq/{sample}.fastq.gz"
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2000
    threads:
        8
    log:
        "log/hisat2/{sample}.hisat2.log"
    benchmark:
        "log/hisat2/{sample}.hisat2.benchmark"
    shell:
        """
        hisat2 -x {input.genome} -p {threads} --dta-cufflinks \
        {params.pe} {params.strand} 2> {log} | \
        samtools sort -@ 2 -o {output.bam}
        samtools index {output.bam}
        """



rule Stringtie:  # todo: group level
    input:
        bam="hisat2/{sample}.bam",
        gtf=config['GTF'],
        strandness='meta/strandness.detected.txt'
    output:
        "stringtie/{sample}.stringtie.gtf"
    conda:
        "envs/stringtie.yaml" 
    params:
        strand=get_strandness_for_stringtie('meta/strandness.detected.txt')
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000
    threads:
        4
    log:
        "log/stringtie/{sample}.stringtie.log"
    benchmark:
        "log/stringtie/{sample}.stringtie.benchmark"
    shell:
        """
        stringtie -G {input.gtf} {params.strand} -o {output} {input.bam} 2> {log}
        """

rule Stringtie_Merge:
    input:
        stringties=expand("stringtie/{sample}.stringtie.gtf", sample=SAMPLES),
        gtf=config['GTF']
    output:
        "stringtie/stringtie.merged.gtf"
    conda:
        "envs/stringtie.yaml" 
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 4000
    threads:
        4
    log:
        "log/stringtie/stringtie.merged.log"
    benchmark:
        "log/stringtie/stringtie.merged.benchmark"
    shell:
        """
        stringtie --merge -G {input.gtf} -o {output} {input.stringties} 2> {log}
        """


rule Create_DAG:
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1000,
    threads:
        1
    output:
        "Workflow_DAG.all.svg"
    log:
        "log/create_dag/Workflow_DAG.all.svg.log"
    envmodules:
        "graphviz/2.26.0"
    shell:
        "snakemake --dag targets > dag 2> {log};"
        "which dot &>> {log};"
        "cat dag|dot -Tsvg > {output} 2>> {log}"


rule reset:
    shell:
        """
        echo 'deleting files..'
        rm -rf fastqc/ bam_qc/ mapped_reads/ sorted_reads/ bam_qc/ bigWig/ \
        feature_count/ fastq_salmon SalmonTE_output/ DESeq2/ gsea/ gsea_compressed/ \
        GATK_ASEReadCounter/ DEXSeq_count/  DEXSeq/ rMATS.*/ \
        _STARgenome _STARtmp \
        feature_count_gene_level hisat2 stringtie \
        lsf.log Log.out nohup.out report.log report.html  dag Workflow_DAG.all.svg \
        meta/strandness.detected.txt  meta/decoder.txt meta/read_length.txt
        echo 'unlocking dir..'
        snakemake -j 1 --unlock
        """

